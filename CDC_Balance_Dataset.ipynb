{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d8b245-5831-4138-9561-9e74eab41b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, count, lit, min, max, mean, stddev\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, NaiveBayes, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "from cdc_module import plot_models_results\n",
    "from training_module import *\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from cdc_module import clean_data\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb89dc55",
   "metadata": {},
   "source": [
    "# **1.Split the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ef773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "cdc_diabetes_health_indicators = fetch_ucirepo(id=891) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = cdc_diabetes_health_indicators.data.features\n",
    "y = cdc_diabetes_health_indicators.data.targets\n",
    "X, y = clean_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8daecab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 14:20:53 WARN Utils: Your hostname, lyudmil-ROG-Zephyrus-M16-GU603ZX-GU603ZX resolves to a loopback address: 127.0.1.1; using 192.168.0.229 instead (on interface wlo1)\n",
      "24/11/07 14:20:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/07 14:20:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/07 14:20:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CDC Diabetes Health Indicators\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\")\\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "X_pyspark = spark.createDataFrame(X)\n",
    "y_pyspark = spark.createDataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3a08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pyspark = X_pyspark.withColumn('Diet', F.col('Fruits') + F.col('Veggies'))\n",
    "X_pyspark = X_pyspark.withColumn('cardiovascular', F.col('HighChol')  + F.col('HighBP'))\n",
    "X_pyspark = X_pyspark.withColumn('unhealthy_behavior', F.col('Smoker') + F.col('HvyAlcoholConsump'))\n",
    "X_pyspark = X_pyspark.withColumn('healthcare',\n",
    "    F.when((F.col('AnyHealthcare') == 1) & (F.col('NoDocbcCost') == 0), 3)\n",
    "     .when((F.col('AnyHealthcare') == 1) & (F.col('NoDocbcCost') == 1), 2)\n",
    "     .when((F.col('AnyHealthcare') == 0) & (F.col('NoDocbcCost') == 0), 1)\n",
    "     .when((F.col('AnyHealthcare') == 0) & (F.col('NoDocbcCost') == 1), 0)\n",
    "     .otherwise(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca61c54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "min_max_features = [\"GenHlth\", \"Age\", \"Education\", \"Income\", \"BMI\", \"MentHlth\", \"PhysHlth\", \"Diet\", \"cardiovascular\", \"unhealthy_behavior\", \"healthcare\"]\n",
    "\n",
    "for feature in min_max_features:\n",
    "    min_val = X_pyspark.agg(min(col(feature))).collect()[0][0]\n",
    "    max_val = X_pyspark.agg(max(col(feature))).collect()[0][0]\n",
    "    X_pyspark = X_pyspark.withColumn(\n",
    "        feature,\n",
    "        (col(feature) - min_val) / (max_val - min_val)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f2aabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes_binary\n",
      "0                  192811\n",
      "1                  192811\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SMOTE object\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to upsample the minority class\n",
    "X_balanced, y_balanced = smote.fit_resample(X_pyspark.toPandas(), y_pyspark.toPandas())\n",
    "\n",
    "# Check the new class distribution\n",
    "y_balanced_counts = y_balanced.value_counts()\n",
    "print(y_balanced_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71a340a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_explained_variance_components_features = ['Fruits','HighChol','HighBP','PhysActivity','Smoker','HeartDiseaseorAttack','Stroke','CholCheck','BMI','Diet','cardiovascular','unhealthy_behavior','healthcare']\n",
    "X_no_compound = [\"Fruits\", \"HighChol\", \"HighBP\", \"PhysActivity\", \"Smoker\", \"Veggies\", \"HeartDiseaseorAttack\", \"Stroke\", \"CholCheck\", \"BMI\"]\n",
    "X_elbow_method_components_features = [\"HighChol\", \"Smoker\", \"HighBP\", \"Stroke\", \"CholCheck\", \"BMI\", \"Diet\", \"cardiovascular\", \"unhealthy_behavior\", \"healthcare\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d120c2d1",
   "metadata": {},
   "source": [
    "## 1. unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1328661",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_explained = X_pyspark[X_explained_variance_components_features]\n",
    "\n",
    "X_elbow = X_pyspark[X_elbow_method_components_features]\n",
    "\n",
    "X_no_compounds = X_pyspark[X_no_compound]\n",
    "\n",
    "# save X_explained_variance_components\n",
    "X_explained.write.mode('overwrite').csv('./X_explained_variance_components_ub.csv', header=True)\n",
    "\n",
    "# save X_elbow_method_components\n",
    "X_elbow.write.mode('overwrite').csv('./X_elbow_method_components_ub.csv', header=True)\n",
    "\n",
    "# save X_no_compound_features\n",
    "X_no_compounds.write.mode('overwrite').csv('./X_no_compound_features_ub.csv', header=True)\n",
    "\n",
    "y_pyspark.write.mode('overwrite').csv('./y_pyspark_ub.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407c7b4",
   "metadata": {},
   "source": [
    "## 2. upsampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68e880fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 X_explained_variance_components.csv\n",
    "X_explained_variance_components = X_balanced[X_explained_variance_components_features]\n",
    "\n",
    "# 读取 X_elbow_method_components.csv\n",
    "X_elbow_method_components = X_balanced[X_elbow_method_components_features]\n",
    "\n",
    "# 读取 X_no_compound_features.csv\n",
    "X_no_compound_features = X_balanced[X_no_compound]\n",
    "\n",
    "\n",
    "X_explained = spark.createDataFrame(X_explained_variance_components)\n",
    "X_elbow = spark.createDataFrame(X_elbow_method_components)\n",
    "X_no_compounds = spark.createDataFrame(X_no_compound_features)\n",
    "y_pyspark = spark.createDataFrame(y_balanced)\n",
    "\n",
    "# save X_explained_variance_components\n",
    "X_explained.write.mode('overwrite').csv('./X_explained_variance_components_up.csv', header=True)\n",
    "\n",
    "# save X_elbow_method_components\n",
    "X_elbow.write.mode('overwrite').csv('./X_elbow_method_components_up.csv', header=True)\n",
    "\n",
    "# save X_no_compound_features\n",
    "X_no_compounds.write.mode('overwrite').csv('./X_no_compound_features_up.csv', header=True)\n",
    "\n",
    "y_pyspark.write.mode('overwrite').csv('./y_pyspark_up.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
