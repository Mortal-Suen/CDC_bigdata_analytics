{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyudmil/anaconda3/envs/big_data/lib/python3.8/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, count, lit, min, max, mean, stddev\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "from pyspark.ml.stat import Correlation, ChiSquareTest\n",
    "from pyspark.ml.feature import VectorAssembler, PCA\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, NaiveBayes, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings coming from Arrow optimizations.\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "cdc_diabetes_health_indicators = fetch_ucirepo(id=891) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = cdc_diabetes_health_indicators.data.features[:100000]\n",
    "y = cdc_diabetes_health_indicators.data.targets[:100000]\n",
    "\n",
    "# Remove duplicate rows\n",
    "combined = pd.concat([X, y], axis=1).drop_duplicates()\n",
    "\n",
    "# Check for identical X with different y and remove them\n",
    "inconsistent_indices = combined[combined.duplicated(subset=combined.columns[:-1], keep=False) & combined.duplicated(subset=[combined.columns[-1]], keep=False)].index\n",
    "if not inconsistent_indices.empty:\n",
    "    combined = combined.drop(inconsistent_indices)\n",
    "\n",
    "# Separate features and target after cleaning\n",
    "X = combined.iloc[:, :-1]\n",
    "y = pd.DataFrame(combined.iloc[:, -1], columns=['Diabetes_binary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/11 15:33:59 WARN Utils: Your hostname, lyudmil-ROG-Zephyrus-M16-GU603ZX-GU603ZX resolves to a loopback address: 127.0.1.1; using 192.168.1.16 instead (on interface wlo1)\n",
      "24/10/11 15:33:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/11 15:33:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HighBP: long (nullable = true)\n",
      " |-- HighChol: long (nullable = true)\n",
      " |-- CholCheck: long (nullable = true)\n",
      " |-- BMI: long (nullable = true)\n",
      " |-- Smoker: long (nullable = true)\n",
      " |-- Stroke: long (nullable = true)\n",
      " |-- HeartDiseaseorAttack: long (nullable = true)\n",
      " |-- PhysActivity: long (nullable = true)\n",
      " |-- Fruits: long (nullable = true)\n",
      " |-- Veggies: long (nullable = true)\n",
      " |-- HvyAlcoholConsump: long (nullable = true)\n",
      " |-- AnyHealthcare: long (nullable = true)\n",
      " |-- NoDocbcCost: long (nullable = true)\n",
      " |-- GenHlth: long (nullable = true)\n",
      " |-- MentHlth: long (nullable = true)\n",
      " |-- PhysHlth: long (nullable = true)\n",
      " |-- DiffWalk: long (nullable = true)\n",
      " |-- Sex: long (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Education: long (nullable = true)\n",
      " |-- Income: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Diabetes_binary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "# Initialize SparkSession\n",
    "conf = SparkConf() \\\n",
    "        .setAppName(\"CDC Diabetes Health Indicators\") \\\n",
    "        .setMaster(\"local[4]\") \\\n",
    "        .set(\"spark.executor.memory\", \"4g\") \\\n",
    "        .set(\"spark.driver.memory\", \"4g\") \\\n",
    "        .set(\"spark.executor.cores\", \"2\")  # Set number of cores per executor\n",
    "\n",
    "# Create Spark session with custom configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# Converting a pandas DataFrame to a PySpark DataFrame\n",
    "# X is the feature, y is the target variable\n",
    "X_pyspark = spark.createDataFrame(X)\n",
    "y_pyspark = spark.createDataFrame(y)\n",
    "\n",
    "# Showing the Architecture of a PySpark DataFrame\n",
    "X_pyspark.printSchema()\n",
    "y_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing Fruits and Veggies to indicate the healthiness of a diet\n",
    "# Summing HighChol and HighBP to indicate overall cardiovascular risk\n",
    "# Summing Smoker and HvyAlcoholConsump to indicate healthcare accessibility\n",
    "\n",
    "X_pyspark = X_pyspark.withColumn('Diet', F.col('Fruits') + F.col('Veggies'))\n",
    "\n",
    "X_pyspark = X_pyspark.withColumn('cardiovascular', F.col('HighChol')  + F.col('HighBP'))\n",
    "\n",
    "X_pyspark = X_pyspark.withColumn('unhealthy_behavior', F.col('Smoker') + F.col('HvyAlcoholConsump'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0：no healthcare and with cost problem。\n",
    "# 1：no healthcare and no cost problem。\n",
    "# 2：with healthcare but with cost problem。\n",
    "# 3：with healthcare and no cost problem。\n",
    "\n",
    "X_pyspark = X_pyspark.withColumn('healthcare',\n",
    "    F.when((F.col('AnyHealthcare') == 1) & (F.col('NoDocbcCost') == 0), 3)\n",
    "     .when((F.col('AnyHealthcare') == 1) & (F.col('NoDocbcCost') == 1), 2)\n",
    "     .when((F.col('AnyHealthcare') == 0) & (F.col('NoDocbcCost') == 0), 1)\n",
    "     .when((F.col('AnyHealthcare') == 0) & (F.col('NoDocbcCost') == 1), 0)\n",
    "     .otherwise(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Min-Max Scaling: GenHlth, Age, Education, Income, BMI, MentHlth, PhysHlth, Diet, cardiovascular, unhealthy_behavior, healthcare\n",
    "min_max_features = [\"GenHlth\", \"Age\", \"Education\", \"Income\", \"BMI\", \"MentHlth\", \"PhysHlth\", \"Diet\", \"cardiovascular\", \"unhealthy_behavior\", \"healthcare\"]\n",
    "\n",
    "for feature in min_max_features:\n",
    "    min_val = X_pyspark.agg(min(col(feature))).collect()[0][0]\n",
    "    max_val = X_pyspark.agg(max(col(feature))).collect()[0][0]\n",
    "    X_pyspark = X_pyspark.withColumn(\n",
    "        feature,\n",
    "        (col(feature) - min_val) / (max_val - min_val)\n",
    "    )\n",
    "\n",
    "# Standardization: BMI, MentHlth, PhysHlth\n",
    "# standardize_features = [\"BMI\", \"MentHlth\", \"PhysHlth\"]\n",
    "\n",
    "# for feature in standardize_features:\n",
    "#     mean_val = X_pyspark.agg(mean(col(feature))).collect()[0][0]\n",
    "#     stddev_val = X_pyspark.agg(stddev(col(feature))).collect()[0][0]\n",
    "#     X_pyspark = X_pyspark.withColumn(\n",
    "#         feature,\n",
    "#         (col(feature) - mean_val) / stddev_val\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pyspark = X_pyspark.join(y_pyspark)\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = data_pyspark.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the features\n",
    "assembler = VectorAssembler(inputCols=X_pyspark.columns, outputCol=\"features\")\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:>  (0 + 4) / 16][Stage 68:>  (0 + 0) / 16][Stage 70:>  (0 + 0) / 16]6]\r"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(labelCol=\"Diabetes_binary\", featuresCol=\"features\")\n",
    "\n",
    "# Param grid for hyperparameter tuning\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation\n",
    "cv_lr = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid_lr,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5, parallelism=4)\n",
    "\n",
    "# Fit model\n",
    "lr_model = cv_lr.fit(train_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
