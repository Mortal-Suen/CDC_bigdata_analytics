{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyudmil/anaconda3/envs/big_data/lib/python3.8/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, count, lit, min, max, mean, stddev\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "from pyspark.ml.stat import Correlation, ChiSquareTest\n",
    "from pyspark.ml.feature import VectorAssembler, PCA\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, NaiveBayes, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings coming from Arrow optimizations.\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "cdc_diabetes_health_indicators = fetch_ucirepo(id=891) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = cdc_diabetes_health_indicators.data.features[:100000]\n",
    "y = cdc_diabetes_health_indicators.data.targets[:100000]\n",
    "\n",
    "# Remove duplicate rows\n",
    "combined = pd.concat([X, y], axis=1).drop_duplicates()\n",
    "\n",
    "# Check for identical X with different y and remove them\n",
    "inconsistent_indices = combined[combined.duplicated(subset=combined.columns[:-1], keep=False) & combined.duplicated(subset=[combined.columns[-1]], keep=False)].index\n",
    "if not inconsistent_indices.empty:\n",
    "    combined = combined.drop(inconsistent_indices)\n",
    "\n",
    "# Separate features and target after cleaning\n",
    "X = combined.iloc[:, :-1]\n",
    "y = pd.DataFrame(combined.iloc[:, -1], columns=['Diabetes_binary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/11 17:51:10 WARN Utils: Your hostname, lyudmil-ROG-Zephyrus-M16-GU603ZX-GU603ZX resolves to a loopback address: 127.0.1.1; using 192.168.1.16 instead (on interface wlo1)\n",
      "24/10/11 17:51:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/11 17:51:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HighBP: long (nullable = true)\n",
      " |-- HighChol: long (nullable = true)\n",
      " |-- CholCheck: long (nullable = true)\n",
      " |-- BMI: long (nullable = true)\n",
      " |-- Smoker: long (nullable = true)\n",
      " |-- Stroke: long (nullable = true)\n",
      " |-- HeartDiseaseorAttack: long (nullable = true)\n",
      " |-- PhysActivity: long (nullable = true)\n",
      " |-- Fruits: long (nullable = true)\n",
      " |-- Veggies: long (nullable = true)\n",
      " |-- HvyAlcoholConsump: long (nullable = true)\n",
      " |-- AnyHealthcare: long (nullable = true)\n",
      " |-- NoDocbcCost: long (nullable = true)\n",
      " |-- GenHlth: long (nullable = true)\n",
      " |-- MentHlth: long (nullable = true)\n",
      " |-- PhysHlth: long (nullable = true)\n",
      " |-- DiffWalk: long (nullable = true)\n",
      " |-- Sex: long (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Education: long (nullable = true)\n",
      " |-- Income: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Diabetes_binary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "# Initialize SparkSession\n",
    "conf = SparkConf() \\\n",
    "        .setAppName(\"CDC Diabetes Health Indicators\") \\\n",
    "        .setMaster(\"local[64]\") \\\n",
    "        .set(\"spark.executor.memory\", \"4g\") \\\n",
    "        .set(\"spark.driver.memory\", \"4g\") \\\n",
    "        .set(\"spark.executor.cores\", \"2\")  # Set number of cores per executor\n",
    "\n",
    "# Create Spark session with custom configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# Converting a pandas DataFrame to a PySpark DataFrame\n",
    "# X is the feature, y is the target variable\n",
    "X_pyspark = spark.createDataFrame(X)\n",
    "y_pyspark = spark.createDataFrame(y)\n",
    "\n",
    "# Showing the Architecture of a PySpark DataFrame\n",
    "X_pyspark.printSchema()\n",
    "y_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USED_FEATURES = ['Diet', 'cardiovascular', 'unhealthy_behavior', 'healthcare', 'Fruits', 'Veggies', 'HighChol', 'HighBP', 'Smoker', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing Fruits and Veggies to indicate the healthiness of a diet\n",
    "# Summing HighChol and HighBP to indicate overall cardiovascular risk\n",
    "# Summing Smoker and HvyAlcoholConsump to indicate healthcare accessibility\n",
    "\n",
    "X_pyspark = X_pyspark.withColumn('Diet', F.col('Fruits') + F.col('Veggies'))\n",
    "\n",
    "X_pyspark = X_pyspark.withColumn('cardiovascular', F.col('HighChol')  + F.col('HighBP'))\n",
    "\n",
    "X_pyspark = X_pyspark.withColumn('unhealthy_behavior', F.col('Smoker') + F.col('HvyAlcoholConsump'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0：no healthcare and with cost problem。\n",
    "# 1：no healthcare and no cost problem。\n",
    "# 2：with healthcare but with cost problem。\n",
    "# 3：with healthcare and no cost problem。\n",
    "\n",
    "X_pyspark = X_pyspark.withColumn('healthcare',\n",
    "    F.when((F.col('AnyHealthcare') == 1) & (F.col('NoDocbcCost') == 0), 3)\n",
    "     .when((F.col('AnyHealthcare') == 1) & (F.col('NoDocbcCost') == 1), 2)\n",
    "     .when((F.col('AnyHealthcare') == 0) & (F.col('NoDocbcCost') == 0), 1)\n",
    "     .when((F.col('AnyHealthcare') == 0) & (F.col('NoDocbcCost') == 1), 0)\n",
    "     .otherwise(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Min-Max Scaling: GenHlth, Age, Education, Income, BMI, MentHlth, PhysHlth, Diet, cardiovascular, unhealthy_behavior, healthcare\n",
    "min_max_features = [\"GenHlth\", \"Age\", \"Education\", \"Income\", \"BMI\", \"MentHlth\", \"PhysHlth\", \"Diet\", \"cardiovascular\", \"unhealthy_behavior\", \"healthcare\"]\n",
    "\n",
    "for feature in min_max_features:\n",
    "    min_val = X_pyspark.agg(min(col(feature))).collect()[0][0]\n",
    "    max_val = X_pyspark.agg(max(col(feature))).collect()[0][0]\n",
    "    X_pyspark = X_pyspark.withColumn(\n",
    "        feature,\n",
    "        (col(feature) - min_val) / (max_val - min_val)\n",
    "    )\n",
    "\n",
    "# Standardization: BMI, MentHlth, PhysHlth\n",
    "# standardize_features = [\"BMI\", \"MentHlth\", \"PhysHlth\"]\n",
    "\n",
    "# for feature in standardize_features:\n",
    "#     mean_val = X_pyspark.agg(mean(col(feature))).collect()[0][0]\n",
    "#     stddev_val = X_pyspark.agg(stddev(col(feature))).collect()[0][0]\n",
    "#     X_pyspark = X_pyspark.withColumn(\n",
    "#         feature,\n",
    "#         (col(feature) - mean_val) / stddev_val\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HighBP',\n",
       " 'HighChol',\n",
       " 'CholCheck',\n",
       " 'BMI',\n",
       " 'Smoker',\n",
       " 'Stroke',\n",
       " 'HeartDiseaseorAttack',\n",
       " 'PhysActivity',\n",
       " 'Fruits',\n",
       " 'Veggies',\n",
       " 'HvyAlcoholConsump',\n",
       " 'AnyHealthcare',\n",
       " 'NoDocbcCost',\n",
       " 'GenHlth',\n",
       " 'MentHlth',\n",
       " 'PhysHlth',\n",
       " 'DiffWalk',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'Education',\n",
       " 'Income']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pyspark.columns[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Diet', 'cardiovascular', 'unhealthy_behavior', 'healthcare']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pyspark.columns[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------+--------------------+------------+-------+-------------------+-------------------+--------+---+-------------------+---------+-------------------+\n",
      "|CholCheck|                BMI|Stroke|HeartDiseaseorAttack|PhysActivity|GenHlth|           MentHlth|           PhysHlth|DiffWalk|Sex|                Age|Education|             Income|\n",
      "+---------+-------------------+------+--------------------+------------+-------+-------------------+-------------------+--------+---+-------------------+---------+-------------------+\n",
      "|        1|0.32558139534883723|     0|                   0|           0|    1.0|                0.6|                0.5|       1|  0| 0.6666666666666666|      0.6| 0.2857142857142857|\n",
      "|        0| 0.1511627906976744|     0|                   0|           1|    0.5|                0.0|                0.0|       0|  0|                0.5|      1.0|                0.0|\n",
      "|        1|0.18604651162790697|     0|                   0|           0|    1.0|                1.0|                1.0|       1|  0| 0.6666666666666666|      0.6|                1.0|\n",
      "|        1| 0.1744186046511628|     0|                   0|           1|   0.25|                0.0|                0.0|       0|  0| 0.8333333333333334|      0.4| 0.7142857142857143|\n",
      "|        1|0.13953488372093023|     0|                   0|           1|   0.25|                0.1|                0.0|       0|  0| 0.8333333333333334|      0.8|0.42857142857142855|\n",
      "|        1| 0.1511627906976744|     0|                   0|           1|   0.25|                0.0|0.06666666666666667|       0|  1|               0.75|      1.0|                1.0|\n",
      "|        1|0.20930232558139536|     0|                   0|           0|    0.5|                0.0| 0.4666666666666667|       0|  0| 0.6666666666666666|      1.0| 0.8571428571428571|\n",
      "|        1| 0.1511627906976744|     0|                   0|           1|    0.5|                0.0|                0.0|       1|  0| 0.8333333333333334|      0.6|0.42857142857142855|\n",
      "|        1|0.20930232558139536|     0|                   1|           0|    1.0|                1.0|                1.0|       1|  0| 0.6666666666666666|      0.8|                0.0|\n",
      "|        1|0.13953488372093023|     0|                   0|           0|   0.25|                0.0|                0.0|       0|  1| 0.5833333333333334|      0.6| 0.2857142857142857|\n",
      "|        1| 0.1511627906976744|     0|                   0|           1|    0.5|                0.0|                0.0|       0|  1|                1.0|      1.0|                1.0|\n",
      "|        1| 0.2558139534883721|     0|                   0|           0|    0.5|                0.0|                1.0|       1|  0|               0.75|      0.8|                0.0|\n",
      "|        1|0.16279069767441862|     0|                   0|           0|    0.5|                0.0|                0.5|       0|  0|                0.5|      0.8| 0.8571428571428571|\n",
      "|        1|0.18604651162790697|     0|                   0|           0|   0.75|                0.0|                0.0|       1|  0| 0.8333333333333334|      0.6| 0.7142857142857143|\n",
      "|        1| 0.2441860465116279|     1|                   0|           1|   0.75|                1.0| 0.9333333333333333|       0|  0|               0.25|      1.0|0.14285714285714285|\n",
      "|        1| 0.2441860465116279|     0|                   0|           1|   0.25|0.16666666666666666|                0.0|       0|  0| 0.4166666666666667|      1.0|                1.0|\n",
      "|        1|0.10465116279069768|     0|                   0|           1|    0.5|                0.0|                0.0|       0|  0|               0.75|      0.6| 0.2857142857142857|\n",
      "|        1|0.12790697674418605|     0|                   0|           1|   0.25|                0.0|                0.0|       0|  1|                0.5|      0.8| 0.7142857142857143|\n",
      "|        0|0.12790697674418605|     0|                   0|           0|   0.25|                0.5|                0.0|       0|  0|0.08333333333333333|      1.0| 0.8571428571428571|\n",
      "|        1|0.18604651162790697|     0|                   0|           0|   0.25| 0.3333333333333333|                0.0|       0|  1|               0.25|      1.0|                1.0|\n",
      "+---------+-------------------+------+--------------------+------------+-------+-------------------+-------------------+--------+---+-------------------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_pyspark_without_used_features = X_pyspark.drop(*USED_FEATURES)\n",
    "X_pyspark_without_used_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=X_pyspark_without_used_features.columns, outputCol=\"features\")\n",
    "X_vector = assembler.transform(X_pyspark).select(\"features\")\n",
    "pca = PCA(k=6, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(X_vector)\n",
    "\n",
    "# Transform the dataset with the PCA model\n",
    "X_pca = pca_model.transform(X_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+\n",
      "|loading             |feature             |principal_component|abs_loading        |\n",
      "+--------------------+--------------------+-------------------+-------------------+\n",
      "|0.9290178552998998  |BMI                 |PC10               |0.9290178552998998 |\n",
      "|0.8360084376199224  |Stroke              |PC5                |0.8360084376199224 |\n",
      "|0.7580561343492052  |PhysActivity        |PC13               |0.7580561343492052 |\n",
      "|0.7276552943594699  |HeartDiseaseorAttack|PC4                |0.7276552943594699 |\n",
      "|0.5144842424645347  |CholCheck           |PC9                |0.5144842424645347 |\n",
      "|-0.4973506109055958 |CholCheck           |PC5                |0.4973506109055958 |\n",
      "|0.4942905799906531  |PhysActivity        |PC9                |0.4942905799906531 |\n",
      "|0.488599594581108   |HeartDiseaseorAttack|PC11               |0.488599594581108  |\n",
      "|0.43826028204976203 |GenHlth             |PC4                |0.43826028204976203|\n",
      "|0.42848771608605435 |GenHlth             |PC7                |0.42848771608605435|\n",
      "|-0.4282965155985866 |GenHlth             |PC9                |0.4282965155985866 |\n",
      "|0.37563242667079316 |GenHlth             |PC8                |0.37563242667079316|\n",
      "|0.3653349504289838  |GenHlth             |PC13               |0.3653349504289838 |\n",
      "|0.35904497420774156 |Stroke              |PC9                |0.35904497420774156|\n",
      "|-0.3429826395704264 |CholCheck           |PC10               |0.3429826395704264 |\n",
      "|0.33874191730846326 |PhysActivity        |PC12               |0.33874191730846326|\n",
      "|0.3362377799253171  |CholCheck           |PC8                |0.3362377799253171 |\n",
      "|-0.3355001801631465 |HeartDiseaseorAttack|PC7                |0.3355001801631465 |\n",
      "|0.304065343438599   |CholCheck           |PC6                |0.304065343438599  |\n",
      "|-0.2947444139607972 |GenHlth             |PC11               |0.2947444139607972 |\n",
      "|-0.2792139784058543 |CholCheck           |PC13               |0.2792139784058543 |\n",
      "|-0.25226614608317816|HeartDiseaseorAttack|PC8                |0.25226614608317816|\n",
      "|0.23649523402854053 |Stroke              |PC8                |0.23649523402854053|\n",
      "|-0.21099093253648402|BMI                 |PC5                |0.21099093253648402|\n",
      "|0.20361568699395646 |GenHlth             |PC6                |0.20361568699395646|\n",
      "|0.19926127437049862 |Stroke              |PC4                |0.19926127437049862|\n",
      "|0.17615588905975527 |Stroke              |PC6                |0.17615588905975527|\n",
      "|0.1724787051758186  |CholCheck           |PC7                |0.1724787051758186 |\n",
      "|0.16765834325822507 |GenHlth             |PC12               |0.16765834325822507|\n",
      "|0.16666813418265128 |BMI                 |PC9                |0.16666813418265128|\n",
      "+--------------------+--------------------+-------------------+-------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming you have the PCA model `pca_model` and the original feature names\n",
    "\n",
    "# Step 1: Extract PCA loadings\n",
    "pca_loadings = pca_model.pc.toArray()  # Eigenvectors for PCA\n",
    "columns = X_pyspark_without_used_features.columns  # Original feature columns\n",
    "\n",
    "# Step 2: Create a DataFrame for PCA loadings\n",
    "loadings_data = [(float(pca_loadings[i][j]), columns[j], f\"PC{i+1}\") for i in range(pca_loadings.shape[0]) for j in range(pca_loadings.shape[1])]\n",
    "loadings_df = spark.createDataFrame(loadings_data, [\"loading\", \"feature\", \"principal_component\"])\n",
    "\n",
    "# Step 3: Calculate absolute contributions\n",
    "loadings_df = loadings_df.withColumn(\"abs_loading\", F.abs(loadings_df[\"loading\"]))\n",
    "\n",
    "# Step 4: Sort by principal component and then by absolute loading\n",
    "sorted_loadings_df = loadings_df.orderBy(\"abs_loading\", ascending=[False])\n",
    "\n",
    "# Show the sorted contributions\n",
    "sorted_loadings_df.show(30, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|feature             |\n",
      "+--------------------+\n",
      "|CholCheck           |\n",
      "|BMI                 |\n",
      "|Stroke              |\n",
      "|HeartDiseaseorAttack|\n",
      "|GenHlth             |\n",
      "|PhysActivity        |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_features = sorted_loadings_df.dropDuplicates([\"feature\"]).select(\"feature\").limit(6)\n",
    "\n",
    "# Show the result\n",
    "result_features.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CholCheck',\n",
       " 'BMI',\n",
       " 'Stroke',\n",
       " 'HeartDiseaseorAttack',\n",
       " 'GenHlth',\n",
       " 'PhysActivity']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features = [row['feature'] for row in result_features.select('feature').collect()]\n",
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_df = X_pca.select('pca_features')\n",
    "\n",
    "# # Define a function to extract the elements of the vector and create new columns\n",
    "# def extract_components(row):\n",
    "#     return [float(x) for x in row[0]]  # Extracting values from the vector\n",
    "\n",
    "# # Use rdd and map to convert the pca_features to separate columns\n",
    "# pca_components = pca_df.rdd.map(extract_components)\n",
    "\n",
    "# # Create a new DataFrame with the principal components\n",
    "# pca_X_pyspark = spark.createDataFrame(pca_components, schema=[f'PC{i+1}' for i in range(4)])\n",
    "\n",
    "# # Show the new DataFrame with principal components\n",
    "# pca_X_pyspark.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------+--------------------+-------+------------+----+--------------+------------------+------------------+\n",
      "|CholCheck|                BMI|Stroke|HeartDiseaseorAttack|GenHlth|PhysActivity|Diet|cardiovascular|unhealthy_behavior|        healthcare|\n",
      "+---------+-------------------+------+--------------------+-------+------------+----+--------------+------------------+------------------+\n",
      "|        1|0.32558139534883723|     0|                   0|    1.0|           0| 0.5|           1.0|               0.5|               1.0|\n",
      "|        0| 0.1511627906976744|     0|                   0|    0.5|           1| 0.0|           0.0|               0.5|               0.0|\n",
      "|        1|0.18604651162790697|     0|                   0|    1.0|           0| 0.5|           1.0|               0.0|0.6666666666666666|\n",
      "|        1| 0.1744186046511628|     0|                   0|   0.25|           1| 1.0|           0.5|               0.0|               1.0|\n",
      "|        1|0.13953488372093023|     0|                   0|   0.25|           1| 1.0|           1.0|               0.0|               1.0|\n",
      "|        1| 0.1511627906976744|     0|                   0|   0.25|           1| 1.0|           1.0|               0.5|               1.0|\n",
      "|        1|0.20930232558139536|     0|                   0|    0.5|           0| 0.0|           0.5|               0.5|               1.0|\n",
      "|        1| 0.1511627906976744|     0|                   0|    0.5|           1| 0.5|           1.0|               0.5|               1.0|\n",
      "|        1|0.20930232558139536|     0|                   1|    1.0|           0| 1.0|           1.0|               0.5|               1.0|\n",
      "|        1|0.13953488372093023|     0|                   0|   0.25|           0| 0.5|           0.0|               0.0|               1.0|\n",
      "|        1| 0.1511627906976744|     0|                   0|    0.5|           1| 1.0|           0.0|               0.5|               1.0|\n",
      "|        1| 0.2558139534883721|     0|                   0|    0.5|           0| 1.0|           1.0|               0.5|               1.0|\n",
      "|        1|0.16279069767441862|     0|                   0|    0.5|           0| 0.5|           0.0|               0.5|               1.0|\n",
      "|        1|0.18604651162790697|     0|                   0|   0.75|           0| 0.5|           1.0|               0.0|               1.0|\n",
      "|        1| 0.2441860465116279|     1|                   0|   0.75|           1| 0.5|           0.5|               0.5|0.6666666666666666|\n",
      "|        1| 0.2441860465116279|     0|                   0|   0.25|           1| 0.0|           0.5|               0.0|               1.0|\n",
      "|        1|0.10465116279069768|     0|                   0|    0.5|           1| 1.0|           1.0|               0.0|               1.0|\n",
      "|        1|0.12790697674418605|     0|                   0|   0.25|           1| 0.0|           0.0|               0.5|               1.0|\n",
      "|        0|0.12790697674418605|     0|                   0|   0.25|           0| 0.5|           0.0|               0.0|               1.0|\n",
      "|        1|0.18604651162790697|     0|                   0|   0.25|           0| 0.0|           0.5|               0.5|               1.0|\n",
      "+---------+-------------------+------+--------------------+-------+------------+----+--------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the desired columns from X_pyspark\n",
    "X_subset = X_pyspark.select(*important_features,'Diet', 'cardiovascular', 'unhealthy_behavior', 'healthcare')\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Show the result\n",
    "X_subset.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pyspark = y_pyspark.withColumn(\"index\", monotonically_increasing_id())\n",
    "X_subset = X_subset.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Perform the join on the index\n",
    "merged_df = X_subset.join(y_pyspark, on=\"index\", how=\"inner\")\n",
    "\n",
    "# Drop the index column if it's no longer needed\n",
    "merged_df = merged_df.drop(\"index\")\n",
    "\n",
    "train_data, test_data = merged_df.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CholCheck',\n",
       " 'BMI',\n",
       " 'Stroke',\n",
       " 'HeartDiseaseorAttack',\n",
       " 'GenHlth',\n",
       " 'PhysActivity',\n",
       " 'Diet',\n",
       " 'cardiovascular',\n",
       " 'unhealthy_behavior',\n",
       " 'healthcare',\n",
       " 'Diabetes_binary']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the features\n",
    "assembler = VectorAssembler(inputCols=merged_df.columns[:-1], outputCol=\"features\")\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+--------------------+-------+------------+----+--------------+------------------+------------------+---------------+--------------------+\n",
      "|CholCheck|                 BMI|Stroke|HeartDiseaseorAttack|GenHlth|PhysActivity|Diet|cardiovascular|unhealthy_behavior|        healthcare|Diabetes_binary|            features|\n",
      "+---------+--------------------+------+--------------------+-------+------------+----+--------------+------------------+------------------+---------------+--------------------+\n",
      "|        0|0.023255813953488372|     0|                   0|    0.5|           1| 1.0|           0.0|               0.5|               1.0|              0|[0.0,0.0232558139...|\n",
      "|        0| 0.03488372093023256|     0|                   1|    0.5|           0| 1.0|           0.0|               0.5|               0.0|              0|(10,[1,3,4,6,8],[...|\n",
      "|        0|0.046511627906976744|     0|                   0|   0.25|           1| 0.0|           0.0|               0.5|               1.0|              0|(10,[1,4,5,8,9],[...|\n",
      "|        0|0.046511627906976744|     0|                   0|   0.25|           1| 0.5|           0.0|               1.0|0.6666666666666666|              0|[0.0,0.0465116279...|\n",
      "|        0|0.046511627906976744|     0|                   0|    0.5|           0| 0.0|           1.0|               0.5|               1.0|              0|(10,[1,4,7,8,9],[...|\n",
      "|        0|0.046511627906976744|     0|                   0|    0.5|           0| 0.5|           0.0|               0.5|0.6666666666666666|              0|(10,[1,4,6,8,9],[...|\n",
      "|        0|0.046511627906976744|     0|                   0|    0.5|           0| 0.5|           0.5|               0.5|               1.0|              0|[0.0,0.0465116279...|\n",
      "|        0|0.046511627906976744|     0|                   0|    0.5|           1| 0.5|           1.0|               0.5|0.6666666666666666|              0|[0.0,0.0465116279...|\n",
      "|        0|0.046511627906976744|     0|                   0|    0.5|           1| 1.0|           0.0|               0.5|               1.0|              0|[0.0,0.0465116279...|\n",
      "|        0|0.046511627906976744|     0|                   0|    1.0|           0| 0.5|           1.0|               0.0|               0.0|              1|(10,[1,4,6,7],[0....|\n",
      "|        0| 0.05813953488372093|     0|                   0|    0.0|           0| 1.0|           0.0|               0.0|0.6666666666666666|              0|(10,[1,6,9],[0.05...|\n",
      "|        0| 0.05813953488372093|     0|                   0|    0.0|           1| 1.0|           0.0|               0.0|               1.0|              0|(10,[1,5,6,9],[0....|\n",
      "|        0| 0.05813953488372093|     0|                   0|    0.0|           1| 1.0|           0.0|               0.0|               1.0|              0|(10,[1,5,6,9],[0....|\n",
      "|        0| 0.05813953488372093|     0|                   0|    0.0|           1| 1.0|           0.0|               0.0|               1.0|              0|(10,[1,5,6,9],[0....|\n",
      "|        0| 0.05813953488372093|     0|                   0|   0.25|           0| 0.0|           0.0|               1.0|               1.0|              0|(10,[1,4,8,9],[0....|\n",
      "|        0| 0.05813953488372093|     0|                   0|   0.25|           0| 0.5|           0.0|               0.0|               1.0|              0|(10,[1,4,6,9],[0....|\n",
      "|        0| 0.05813953488372093|     0|                   0|   0.25|           1| 0.5|           0.0|               1.0|               1.0|              0|[0.0,0.0581395348...|\n",
      "|        0| 0.05813953488372093|     0|                   0|   0.25|           1| 1.0|           0.0|               0.0|0.3333333333333333|              0|(10,[1,4,5,6,9],[...|\n",
      "|        0| 0.05813953488372093|     0|                   0|   0.25|           1| 1.0|           0.0|               0.0|               1.0|              0|(10,[1,4,5,6,9],[...|\n",
      "|        0| 0.05813953488372093|     0|                   0|   0.25|           1| 1.0|           0.0|               0.5|               0.0|              0|(10,[1,4,5,6,8],[...|\n",
      "+---------+--------------------+------+--------------------+-------+------------+----+--------------+------------------+------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CholCheck',\n",
       " 'BMI',\n",
       " 'Stroke',\n",
       " 'HeartDiseaseorAttack',\n",
       " 'GenHlth',\n",
       " 'PhysActivity',\n",
       " 'Diet',\n",
       " 'cardiovascular',\n",
       " 'unhealthy_behavior',\n",
       " 'healthcare',\n",
       " 'Diabetes_binary',\n",
       " 'features']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def get_hyperparameters(model):\n",
    "    best_model = model.bestModel\n",
    "\n",
    "    # Extract and print the best hyperparameters\n",
    "    best_params = best_model.extractParamMap()\n",
    "\n",
    "    # Print each parameter and its value\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param.name}: {value}\")\n",
    "\n",
    "        \n",
    "def get_metrics(predictions):\n",
    "    # Accuracy\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"Diabetes_binary\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "\n",
    "    # F1-Score\n",
    "    f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"Diabetes_binary\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "    f1_score = f1_evaluator.evaluate(predictions)\n",
    "\n",
    "    # Recall\n",
    "    recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"Diabetes_binary\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "    recall = recall_evaluator.evaluate(predictions)\n",
    "\n",
    "    # Precision\n",
    "    precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"Diabetes_binary\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "    precision = precision_evaluator.evaluate(predictions)\n",
    "\n",
    "    # Step 3: Print the evaluation results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-Score: {f1_score:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                00]\r"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(labelCol=\"Diabetes_binary\", featuresCol=\"features\")\n",
    "\n",
    "# Param grid for hyperparameter tuning\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation\n",
    "cv_lr = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid_lr,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5, parallelism=64)\n",
    "\n",
    "# Fit model\n",
    "lr_model = cv_lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: 2\n",
      "elasticNetParam: 0.0\n",
      "family: auto\n",
      "featuresCol: features\n",
      "fitIntercept: True\n",
      "labelCol: Diabetes_binary\n",
      "maxBlockSizeInMB: 0.0\n",
      "maxIter: 100\n",
      "predictionCol: prediction\n",
      "probabilityCol: probability\n",
      "rawPredictionCol: rawPrediction\n",
      "regParam: 0.01\n",
      "standardization: True\n",
      "threshold: 0.5\n",
      "tol: 1e-06\n"
     ]
    }
   ],
   "source": [
    "get_hyperparameters(lr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8615\n",
      "F1-Score: 0.8217\n",
      "Recall: 0.8615\n",
      "Precision: 0.8246\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_data)\n",
    "get_metrics(lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                0]0]]]\r"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"Diabetes_binary\", featuresCol=\"features\")\n",
    "\n",
    "paramGrid_dt = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 20]) \\\n",
    "    .addGrid(dt.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "cv_dt = CrossValidator(estimator=dt,\n",
    "                    estimatorParamMaps=paramGrid_dt,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5, parallelism=64)\n",
    "\n",
    "dt_model = cv_dt.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: False\n",
      "checkpointInterval: 10\n",
      "featuresCol: features\n",
      "impurity: gini\n",
      "labelCol: Diabetes_binary\n",
      "leafCol: \n",
      "maxBins: 64\n",
      "maxDepth: 10\n",
      "maxMemoryInMB: 256\n",
      "minInfoGain: 0.0\n",
      "minInstancesPerNode: 1\n",
      "minWeightFractionPerNode: 0.0\n",
      "predictionCol: prediction\n",
      "probabilityCol: probability\n",
      "rawPredictionCol: rawPrediction\n",
      "seed: 8624640193437594967\n"
     ]
    }
   ],
   "source": [
    "get_hyperparameters(dt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8603\n",
      "F1-Score: 0.8258\n",
      "Recall: 0.8603\n",
      "Precision: 0.8242\n"
     ]
    }
   ],
   "source": [
    "dt_predictions = dt_model.transform(test_data)\n",
    "get_metrics(dt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Random Forest Classifier\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rf \u001b[38;5;241m=\u001b[39m \u001b[43mRandomForestClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDiabetes_binary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m paramGrid_rf \u001b[38;5;241m=\u001b[39m ParamGridBuilder() \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39maddGrid(rf\u001b[38;5;241m.\u001b[39mnumTrees, [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m]) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39maddGrid(rf\u001b[38;5;241m.\u001b[39mmaxDepth, [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m]) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m      9\u001b[0m cv_rf \u001b[38;5;241m=\u001b[39m CrossValidator(estimator\u001b[38;5;241m=\u001b[39mrf,\n\u001b[1;32m     10\u001b[0m                     estimatorParamMaps\u001b[38;5;241m=\u001b[39mparamGrid_rf,\n\u001b[1;32m     11\u001b[0m                     evaluator\u001b[38;5;241m=\u001b[39mBinaryClassificationEvaluator(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiabetes_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     12\u001b[0m                     numFolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, parallelism\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/big_data/lib/python3.8/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/big_data/lib/python3.8/site-packages/pyspark/ml/classification.py:2104\u001b[0m, in \u001b[0;36mRandomForestClassifier.__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, probabilityCol, rawPredictionCol, maxDepth, maxBins, minInstancesPerNode, minInfoGain, maxMemoryInMB, cacheNodeIds, checkpointInterval, impurity, numTrees, featureSubsetStrategy, seed, subsamplingRate, leafCol, minWeightFractionPerNode, weightCol, bootstrap)\u001b[0m\n\u001b[1;32m   2095\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;124;03m__init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;124;03m         probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;124;03m         leafCol=\"\", minWeightFractionPerNode=0.0, weightCol=None, bootstrap=True)\u001b[39;00m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;28msuper\u001b[39m(RandomForestClassifier, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m-> 2104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.ml.classification.RandomForestClassifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\n\u001b[1;32m   2106\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2107\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/big_data/lib/python3.8/site-packages/pyspark/ml/wrapper.py:84\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     82\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m _jvm()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjava_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_obj(\u001b[38;5;241m*\u001b[39mjava_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/big_data/lib/python3.8/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/anaconda3/envs/big_data/lib/python3.8/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/anaconda3/envs/big_data/lib/python3.8/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/anaconda3/envs/big_data/lib/python3.8/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/anaconda3/envs/big_data/lib/python3.8/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(labelCol=\"Diabetes_binary\", featuresCol=\"features\")\n",
    "\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "cv_rf = CrossValidator(estimator=rf,\n",
    "                    estimatorParamMaps=paramGrid_rf,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5, parallelism=32)\n",
    "\n",
    "rf_model = cv_rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hyperparameters(rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions = rf_model.transform(test_data)\n",
    "get_metrics(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                00]]]0]\r"
     ]
    }
   ],
   "source": [
    "# Gradient-Boosted Tree Classifier\n",
    "gbt = GBTClassifier(labelCol=\"Diabetes_binary\", featuresCol=\"features\", maxIter=100)\n",
    "\n",
    "paramGrid_gbt = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "cv_gbt = CrossValidator(estimator=gbt,\n",
    "                    estimatorParamMaps=paramGrid_gbt,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5, parallelism=64)\n",
    "\n",
    "gbt_model = cv_gbt.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hyperparameters(gbt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "get_metrics(gbt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svm = LinearSVC(labelCol=\"Diabetes_binary\", featuresCol=\"features\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(svm.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(estimator=svm,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5,  parallelism=64)\n",
    "\n",
    "svm_model = cv.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hyperparameters(svm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm_model.transform(test_data)\n",
    "get_metrics(svm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "nb = NaiveBayes(labelCol=\"Diabetes_binary\", featuresCol=\"features\")\n",
    "\n",
    "paramGrid_nb = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0.5, 1.0, 1.5]) \\\n",
    "    .build()\n",
    "\n",
    "cv_nb = CrossValidator(estimator=nb,\n",
    "                    estimatorParamMaps=paramGrid_nb,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5,  parallelism=64)\n",
    "\n",
    "nb_model = cv_nb.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hyperparameters(nb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predictions = nb_model.transform(test_data)\n",
    "get_metrics(nb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP classifier\n",
    "layers = 3\n",
    "mlp = MultilayerPerceptronClassifier(labelCol=\"Diabetes_binary\", featuresCol=\"features\", layers=layers, seed=1234)\n",
    "\n",
    "paramGrid_mlp = ParamGridBuilder() \\\n",
    "    .addGrid(mlp.layers, [[len(X_pyspark.columns), 5, 4, 2], [len(X_pyspark.columns), 10, 5, 2]]) \\\n",
    "    .build()\n",
    "\n",
    "cv_mlp = CrossValidator(estimator=mlp,\n",
    "                    estimatorParamMaps=paramGrid_mlp,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5,  parallelism=64)\n",
    "\n",
    "mlp_model = cv_mlp.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hyperparameters(mlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_predictions = mlp_model.transform(test_data)\n",
    "get_metrics(mlp_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
