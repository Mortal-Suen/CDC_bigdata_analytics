{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, count, lit, min, max, mean, stddev\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "from pyspark.ml.stat import Correlation, ChiSquareTest\n",
    "from pyspark.ml.feature import VectorAssembler, PCA\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, NaiveBayes, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings coming from Arrow optimizations.\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "cdc_diabetes_health_indicators = fetch_ucirepo(id=891) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = cdc_diabetes_health_indicators.data.features[:100000]\n",
    "y = cdc_diabetes_health_indicators.data.targets[:100000]\n",
    "\n",
    "# Remove duplicate rows\n",
    "combined = pd.concat([X, y], axis=1).drop_duplicates()\n",
    "\n",
    "# Check for identical X with different y and remove them\n",
    "inconsistent_indices = combined[combined.duplicated(subset=combined.columns[:-1], keep=False) & combined.duplicated(subset=[combined.columns[-1]], keep=False)].index\n",
    "if not inconsistent_indices.empty:\n",
    "    combined = combined.drop(inconsistent_indices)\n",
    "\n",
    "# Separate features and target after cleaning\n",
    "X = combined.iloc[:, :-1]\n",
    "y = pd.DataFrame(combined.iloc[:, -1], columns=['Diabetes_binary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HighBP: long (nullable = true)\n",
      " |-- HighChol: long (nullable = true)\n",
      " |-- CholCheck: long (nullable = true)\n",
      " |-- BMI: long (nullable = true)\n",
      " |-- Smoker: long (nullable = true)\n",
      " |-- Stroke: long (nullable = true)\n",
      " |-- HeartDiseaseorAttack: long (nullable = true)\n",
      " |-- PhysActivity: long (nullable = true)\n",
      " |-- Fruits: long (nullable = true)\n",
      " |-- Veggies: long (nullable = true)\n",
      " |-- HvyAlcoholConsump: long (nullable = true)\n",
      " |-- AnyHealthcare: long (nullable = true)\n",
      " |-- NoDocbcCost: long (nullable = true)\n",
      " |-- GenHlth: long (nullable = true)\n",
      " |-- MentHlth: long (nullable = true)\n",
      " |-- PhysHlth: long (nullable = true)\n",
      " |-- DiffWalk: long (nullable = true)\n",
      " |-- Sex: long (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Education: long (nullable = true)\n",
      " |-- Income: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Diabetes_binary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "# Initialize SparkSession\n",
    "conf = SparkConf() \\\n",
    "        .setAppName(\"CDC Diabetes Health Indicators\") \\\n",
    "        .setMaster(\"local[64]\") \\\n",
    "        .set(\"spark.executor.memory\", \"4g\") \\\n",
    "        .set(\"spark.driver.memory\", \"4g\") \\\n",
    "        .set(\"spark.executor.cores\", \"2\")  # Set number of cores per executor\n",
    "\n",
    "# Create Spark session with custom configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# Converting a pandas DataFrame to a PySpark DataFrame\n",
    "# X is the feature, y is the target variable\n",
    "X_pyspark = spark.createDataFrame(X)\n",
    "y_pyspark = spark.createDataFrame(y)\n",
    "\n",
    "# Showing the Architecture of a PySpark DataFrame\n",
    "X_pyspark.printSchema()\n",
    "y_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "USED_FEATURES = ['Diet', 'cardiovascular', 'unhealthy_behavior', 'healthcare', 'Fruits', 'Veggies', 'HighChol', 'HighBP', 'Smoker', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing Fruits and Veggies to indicate the healthiness of a diet\n",
    "# Summing HighChol and HighBP to indicate overall cardiovascular risk\n",
    "# Summing Smoker and HvyAlcoholConsump to indicate healthcare accessibility\n",
    "\n",
    "X_pyspark = X_pyspark.withColumn('Diet', F.col('Fruits') + F.col('Veggies'))\n",
    "\n",
    "X_pyspark = X_pyspark.withColumn('cardiovascular', F.col('HighChol')  + F.col('HighBP'))\n",
    "\n",
    "X_pyspark = X_pyspark.withColumn('unhealthy_behavior', F.col('Smoker') + F.col('HvyAlcoholConsump'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0：no healthcare and with cost problem。\n",
    "# 1：no healthcare and no cost problem。\n",
    "# 2：with healthcare but with cost problem。\n",
    "# 3：with healthcare and no cost problem。\n",
    "\n",
    "X_pyspark = X_pyspark.withColumn('healthcare',\n",
    "    F.when((F.col('AnyHealthcare') == 1) & (F.col('NoDocbcCost') == 0), 3)\n",
    "     .when((F.col('AnyHealthcare') == 1) & (F.col('NoDocbcCost') == 1), 2)\n",
    "     .when((F.col('AnyHealthcare') == 0) & (F.col('NoDocbcCost') == 0), 1)\n",
    "     .when((F.col('AnyHealthcare') == 0) & (F.col('NoDocbcCost') == 1), 0)\n",
    "     .otherwise(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Scaling: GenHlth, Age, Education, Income, BMI, MentHlth, PhysHlth, Diet, cardiovascular, unhealthy_behavior, healthcare\n",
    "min_max_features = [\"GenHlth\", \"Age\", \"Education\", \"Income\", \"BMI\", \"MentHlth\", \"PhysHlth\", \"Diet\", \"cardiovascular\", \"unhealthy_behavior\", \"healthcare\"]\n",
    "\n",
    "for feature in min_max_features:\n",
    "    min_val = X_pyspark.agg(min(col(feature))).collect()[0][0]\n",
    "    max_val = X_pyspark.agg(max(col(feature))).collect()[0][0]\n",
    "    X_pyspark = X_pyspark.withColumn(\n",
    "        feature,\n",
    "        (col(feature) - min_val) / (max_val - min_val)\n",
    "    )\n",
    "\n",
    "# Standardization: BMI, MentHlth, PhysHlth\n",
    "# standardize_features = [\"BMI\", \"MentHlth\", \"PhysHlth\"]\n",
    "\n",
    "# for feature in standardize_features:\n",
    "#     mean_val = X_pyspark.agg(mean(col(feature))).collect()[0][0]\n",
    "#     stddev_val = X_pyspark.agg(stddev(col(feature))).collect()[0][0]\n",
    "#     X_pyspark = X_pyspark.withColumn(\n",
    "#         feature,\n",
    "#         (col(feature) - mean_val) / stddev_val\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HighBP',\n",
       " 'HighChol',\n",
       " 'CholCheck',\n",
       " 'BMI',\n",
       " 'Smoker',\n",
       " 'Stroke',\n",
       " 'HeartDiseaseorAttack',\n",
       " 'PhysActivity',\n",
       " 'Fruits',\n",
       " 'Veggies',\n",
       " 'HvyAlcoholConsump',\n",
       " 'AnyHealthcare',\n",
       " 'NoDocbcCost',\n",
       " 'GenHlth',\n",
       " 'MentHlth',\n",
       " 'PhysHlth',\n",
       " 'DiffWalk',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'Education',\n",
       " 'Income']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pyspark.columns[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Diet', 'cardiovascular', 'unhealthy_behavior', 'healthcare']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pyspark.columns[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------+--------------------+------------+-------+-------------------+-------------------+--------+---+-------------------+---------+-------------------+\n",
      "|CholCheck|                BMI|Stroke|HeartDiseaseorAttack|PhysActivity|GenHlth|           MentHlth|           PhysHlth|DiffWalk|Sex|                Age|Education|             Income|\n",
      "+---------+-------------------+------+--------------------+------------+-------+-------------------+-------------------+--------+---+-------------------+---------+-------------------+\n",
      "|        1|0.32558139534883723|     0|                   0|           0|    1.0|                0.6|                0.5|       1|  0| 0.6666666666666666|      0.6| 0.2857142857142857|\n",
      "|        0| 0.1511627906976744|     0|                   0|           1|    0.5|                0.0|                0.0|       0|  0|                0.5|      1.0|                0.0|\n",
      "|        1|0.18604651162790697|     0|                   0|           0|    1.0|                1.0|                1.0|       1|  0| 0.6666666666666666|      0.6|                1.0|\n",
      "|        1| 0.1744186046511628|     0|                   0|           1|   0.25|                0.0|                0.0|       0|  0| 0.8333333333333334|      0.4| 0.7142857142857143|\n",
      "|        1|0.13953488372093023|     0|                   0|           1|   0.25|                0.1|                0.0|       0|  0| 0.8333333333333334|      0.8|0.42857142857142855|\n",
      "|        1| 0.1511627906976744|     0|                   0|           1|   0.25|                0.0|0.06666666666666667|       0|  1|               0.75|      1.0|                1.0|\n",
      "|        1|0.20930232558139536|     0|                   0|           0|    0.5|                0.0| 0.4666666666666667|       0|  0| 0.6666666666666666|      1.0| 0.8571428571428571|\n",
      "|        1| 0.1511627906976744|     0|                   0|           1|    0.5|                0.0|                0.0|       1|  0| 0.8333333333333334|      0.6|0.42857142857142855|\n",
      "|        1|0.20930232558139536|     0|                   1|           0|    1.0|                1.0|                1.0|       1|  0| 0.6666666666666666|      0.8|                0.0|\n",
      "|        1|0.13953488372093023|     0|                   0|           0|   0.25|                0.0|                0.0|       0|  1| 0.5833333333333334|      0.6| 0.2857142857142857|\n",
      "|        1| 0.1511627906976744|     0|                   0|           1|    0.5|                0.0|                0.0|       0|  1|                1.0|      1.0|                1.0|\n",
      "|        1| 0.2558139534883721|     0|                   0|           0|    0.5|                0.0|                1.0|       1|  0|               0.75|      0.8|                0.0|\n",
      "|        1|0.16279069767441862|     0|                   0|           0|    0.5|                0.0|                0.5|       0|  0|                0.5|      0.8| 0.8571428571428571|\n",
      "|        1|0.18604651162790697|     0|                   0|           0|   0.75|                0.0|                0.0|       1|  0| 0.8333333333333334|      0.6| 0.7142857142857143|\n",
      "|        1| 0.2441860465116279|     1|                   0|           1|   0.75|                1.0| 0.9333333333333333|       0|  0|               0.25|      1.0|0.14285714285714285|\n",
      "|        1| 0.2441860465116279|     0|                   0|           1|   0.25|0.16666666666666666|                0.0|       0|  0| 0.4166666666666667|      1.0|                1.0|\n",
      "|        1|0.10465116279069768|     0|                   0|           1|    0.5|                0.0|                0.0|       0|  0|               0.75|      0.6| 0.2857142857142857|\n",
      "|        1|0.12790697674418605|     0|                   0|           1|   0.25|                0.0|                0.0|       0|  1|                0.5|      0.8| 0.7142857142857143|\n",
      "|        0|0.12790697674418605|     0|                   0|           0|   0.25|                0.5|                0.0|       0|  0|0.08333333333333333|      1.0| 0.8571428571428571|\n",
      "|        1|0.18604651162790697|     0|                   0|           0|   0.25| 0.3333333333333333|                0.0|       0|  1|               0.25|      1.0|                1.0|\n",
      "+---------+-------------------+------+--------------------+------------+-------+-------------------+-------------------+--------+---+-------------------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_pyspark_without_used_features = X_pyspark.drop(*USED_FEATURES)\n",
    "X_pyspark_without_used_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=X_pyspark_without_used_features.columns, outputCol=\"features\")\n",
    "X_vector = assembler.transform(X_pyspark).select(\"features\")\n",
    "pca = PCA(k=6, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(X_vector)\n",
    "\n",
    "# Transform the dataset with the PCA model\n",
    "X_pca = pca_model.transform(X_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+\n",
      "|loading             |feature             |principal_component|abs_loading        |\n",
      "+--------------------+--------------------+-------------------+-------------------+\n",
      "|0.9290178552999001  |BMI                 |PC10               |0.9290178552999001 |\n",
      "|0.8360084376199223  |Stroke              |PC5                |0.8360084376199223 |\n",
      "|0.7580561343492066  |PhysActivity        |PC13               |0.7580561343492066 |\n",
      "|0.72765529435947    |HeartDiseaseorAttack|PC4                |0.72765529435947   |\n",
      "|0.514484242464535   |CholCheck           |PC9                |0.514484242464535  |\n",
      "|-0.4973506109055956 |CholCheck           |PC5                |0.4973506109055956 |\n",
      "|0.49429057999065257 |PhysActivity        |PC9                |0.49429057999065257|\n",
      "|0.48859959458110847 |HeartDiseaseorAttack|PC11               |0.48859959458110847|\n",
      "|0.4382602820497641  |GenHlth             |PC4                |0.4382602820497641 |\n",
      "|0.4284877160860541  |GenHlth             |PC7                |0.4284877160860541 |\n",
      "|-0.42829651559858695|GenHlth             |PC9                |0.42829651559858695|\n",
      "|0.3756324266707926  |GenHlth             |PC8                |0.3756324266707926 |\n",
      "|0.3653349504289832  |GenHlth             |PC13               |0.3653349504289832 |\n",
      "|0.3590449742077418  |Stroke              |PC9                |0.3590449742077418 |\n",
      "|-0.34298263957042635|CholCheck           |PC10               |0.34298263957042635|\n",
      "|0.33874191730846454 |PhysActivity        |PC12               |0.33874191730846454|\n",
      "|0.3362377799253172  |CholCheck           |PC8                |0.3362377799253172 |\n",
      "|-0.33550018016314787|HeartDiseaseorAttack|PC7                |0.33550018016314787|\n",
      "|0.3040653434385989  |CholCheck           |PC6                |0.3040653434385989 |\n",
      "|-0.2947444139607973 |GenHlth             |PC11               |0.2947444139607973 |\n",
      "|-0.27921397840585427|CholCheck           |PC13               |0.27921397840585427|\n",
      "|-0.25226614608317965|HeartDiseaseorAttack|PC8                |0.25226614608317965|\n",
      "|0.23649523402854034 |Stroke              |PC8                |0.23649523402854034|\n",
      "|-0.21099093253648357|BMI                 |PC5                |0.21099093253648357|\n",
      "|0.20361568699395657 |GenHlth             |PC6                |0.20361568699395657|\n",
      "|0.19926127437049918 |Stroke              |PC4                |0.19926127437049918|\n",
      "|0.17615588905975504 |Stroke              |PC6                |0.17615588905975504|\n",
      "|0.17247870517581854 |CholCheck           |PC7                |0.17247870517581854|\n",
      "|0.1676583432582246  |GenHlth             |PC12               |0.1676583432582246 |\n",
      "|0.1666681341826514  |BMI                 |PC9                |0.1666681341826514 |\n",
      "+--------------------+--------------------+-------------------+-------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming you have the PCA model `pca_model` and the original feature names\n",
    "\n",
    "# Step 1: Extract PCA loadings\n",
    "pca_loadings = pca_model.pc.toArray()  # Eigenvectors for PCA\n",
    "columns = X_pyspark_without_used_features.columns  # Original feature columns\n",
    "\n",
    "# Step 2: Create a DataFrame for PCA loadings\n",
    "loadings_data = [(float(pca_loadings[i][j]), columns[j], f\"PC{i+1}\") for i in range(pca_loadings.shape[0]) for j in range(pca_loadings.shape[1])]\n",
    "loadings_df = spark.createDataFrame(loadings_data, [\"loading\", \"feature\", \"principal_component\"])\n",
    "\n",
    "# Step 3: Calculate absolute contributions\n",
    "loadings_df = loadings_df.withColumn(\"abs_loading\", F.abs(loadings_df[\"loading\"]))\n",
    "\n",
    "# Step 4: Sort by principal component and then by absolute loading\n",
    "sorted_loadings_df = loadings_df.orderBy(\"abs_loading\", ascending=[False])\n",
    "\n",
    "# Show the sorted contributions\n",
    "sorted_loadings_df.show(30, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|feature             |\n",
      "+--------------------+\n",
      "|CholCheck           |\n",
      "|BMI                 |\n",
      "|Stroke              |\n",
      "|HeartDiseaseorAttack|\n",
      "|GenHlth             |\n",
      "|PhysActivity        |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_features = sorted_loadings_df.dropDuplicates([\"feature\"]).select(\"feature\").limit(6)\n",
    "\n",
    "# Show the result\n",
    "result_features.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CholCheck',\n",
       " 'BMI',\n",
       " 'Stroke',\n",
       " 'HeartDiseaseorAttack',\n",
       " 'GenHlth',\n",
       " 'PhysActivity']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features = [row['feature'] for row in result_features.select('feature').collect()]\n",
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_df = X_pca.select('pca_features')\n",
    "\n",
    "# # Define a function to extract the elements of the vector and create new columns\n",
    "# def extract_components(row):\n",
    "#     return [float(x) for x in row[0]]  # Extracting values from the vector\n",
    "\n",
    "# # Use rdd and map to convert the pca_features to separate columns\n",
    "# pca_components = pca_df.rdd.map(extract_components)\n",
    "\n",
    "# # Create a new DataFrame with the principal components\n",
    "# pca_X_pyspark = spark.createDataFrame(pca_components, schema=[f'PC{i+1}' for i in range(4)])\n",
    "\n",
    "# # Show the new DataFrame with principal components\n",
    "# pca_X_pyspark.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------+--------------------+-------+------------+----+--------------+------------------+------------------+\n",
      "|CholCheck|                BMI|Stroke|HeartDiseaseorAttack|GenHlth|PhysActivity|Diet|cardiovascular|unhealthy_behavior|        healthcare|\n",
      "+---------+-------------------+------+--------------------+-------+------------+----+--------------+------------------+------------------+\n",
      "|        1|0.32558139534883723|     0|                   0|    1.0|           0| 0.5|           1.0|               0.5|               1.0|\n",
      "|        0| 0.1511627906976744|     0|                   0|    0.5|           1| 0.0|           0.0|               0.5|               0.0|\n",
      "|        1|0.18604651162790697|     0|                   0|    1.0|           0| 0.5|           1.0|               0.0|0.6666666666666666|\n",
      "|        1| 0.1744186046511628|     0|                   0|   0.25|           1| 1.0|           0.5|               0.0|               1.0|\n",
      "|        1|0.13953488372093023|     0|                   0|   0.25|           1| 1.0|           1.0|               0.0|               1.0|\n",
      "|        1| 0.1511627906976744|     0|                   0|   0.25|           1| 1.0|           1.0|               0.5|               1.0|\n",
      "|        1|0.20930232558139536|     0|                   0|    0.5|           0| 0.0|           0.5|               0.5|               1.0|\n",
      "|        1| 0.1511627906976744|     0|                   0|    0.5|           1| 0.5|           1.0|               0.5|               1.0|\n",
      "|        1|0.20930232558139536|     0|                   1|    1.0|           0| 1.0|           1.0|               0.5|               1.0|\n",
      "|        1|0.13953488372093023|     0|                   0|   0.25|           0| 0.5|           0.0|               0.0|               1.0|\n",
      "|        1| 0.1511627906976744|     0|                   0|    0.5|           1| 1.0|           0.0|               0.5|               1.0|\n",
      "|        1| 0.2558139534883721|     0|                   0|    0.5|           0| 1.0|           1.0|               0.5|               1.0|\n",
      "|        1|0.16279069767441862|     0|                   0|    0.5|           0| 0.5|           0.0|               0.5|               1.0|\n",
      "|        1|0.18604651162790697|     0|                   0|   0.75|           0| 0.5|           1.0|               0.0|               1.0|\n",
      "|        1| 0.2441860465116279|     1|                   0|   0.75|           1| 0.5|           0.5|               0.5|0.6666666666666666|\n",
      "|        1| 0.2441860465116279|     0|                   0|   0.25|           1| 0.0|           0.5|               0.0|               1.0|\n",
      "|        1|0.10465116279069768|     0|                   0|    0.5|           1| 1.0|           1.0|               0.0|               1.0|\n",
      "|        1|0.12790697674418605|     0|                   0|   0.25|           1| 0.0|           0.0|               0.5|               1.0|\n",
      "|        0|0.12790697674418605|     0|                   0|   0.25|           0| 0.5|           0.0|               0.0|               1.0|\n",
      "|        1|0.18604651162790697|     0|                   0|   0.25|           0| 0.0|           0.5|               0.5|               1.0|\n",
      "+---------+-------------------+------+--------------------+-------+------------+----+--------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the desired columns from X_pyspark\n",
    "X_subset = X_pyspark.select(*important_features,'Diet', 'cardiovascular', 'unhealthy_behavior', 'healthcare')\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Show the result\n",
    "X_subset.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pyspark = y_pyspark.withColumn(\"index\", monotonically_increasing_id())\n",
    "X_subset = X_subset.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Perform the join on the index\n",
    "merged_df = X_subset.join(y_pyspark, on=\"index\", how=\"inner\")\n",
    "\n",
    "# Drop the index column if it's no longer needed\n",
    "merged_df = merged_df.drop(\"index\")\n",
    "\n",
    "train_data, test_data = merged_df.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CholCheck',\n",
       " 'BMI',\n",
       " 'Stroke',\n",
       " 'HeartDiseaseorAttack',\n",
       " 'GenHlth',\n",
       " 'PhysActivity',\n",
       " 'Diet',\n",
       " 'cardiovascular',\n",
       " 'unhealthy_behavior',\n",
       " 'healthcare',\n",
       " 'Diabetes_binary']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the features\n",
    "assembler = VectorAssembler(inputCols=merged_df.columns[:-1], outputCol=\"features\")\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+--------------------+-------+------------+----+--------------+------------------+------------------+---------------+--------------------+\n",
      "|CholCheck|                 BMI|Stroke|HeartDiseaseorAttack|GenHlth|PhysActivity|Diet|cardiovascular|unhealthy_behavior|        healthcare|Diabetes_binary|            features|\n",
      "+---------+--------------------+------+--------------------+-------+------------+----+--------------+------------------+------------------+---------------+--------------------+\n",
      "|        0|0.023255813953488372|     0|                   0|    0.5|           1| 1.0|           0.0|               0.5|               1.0|              0|[0.0,0.0232558139...|\n",
      "|        0| 0.03488372093023256|     0|                   1|    0.5|           0| 1.0|           0.0|               0.5|               0.0|              0|(10,[1,3,4,6,8],[...|\n",
      "|        0|0.046511627906976744|     0|                   0|   0.25|           1| 0.0|           0.0|               0.5|               1.0|              0|(10,[1,4,5,8,9],[...|\n",
      "|        0|0.046511627906976744|     0|                   0|   0.25|           1| 0.5|           0.0|               1.0|0.6666666666666666|              0|[0.0,0.0465116279...|\n",
      "|        0|0.046511627906976744|     0|                   0|    0.5|           0| 0.0|           1.0|               0.5|               1.0|              0|(10,[1,4,7,8,9],[...|\n",
      "|        0|0.046511627906976744|     0|                   0|    0.5|           0| 0.5|           0.0|               0.5|0.6666666666666666|              0|(10,[1,4,6,8,9],[...|\n",
      "|        0|0.046511627906976744|     0|                   0|    0.5|           0| 0.5|           0.5|               0.5|               1.0|              0|[0.0,0.0465116279...|\n",
      "|        0|0.046511627906976744|     0|                   0|    0.5|           1| 0.5|           1.0|               0.5|0.6666666666666666|              0|[0.0,0.0465116279...|\n",
      "|        0|0.046511627906976744|     0|                   0|    0.5|           1| 1.0|           0.0|               0.5|               1.0|              0|[0.0,0.0465116279...|\n",
      "|        0|0.046511627906976744|     0|                   0|    1.0|           0| 0.5|           1.0|               0.0|               0.0|              1|(10,[1,4,6,7],[0....|\n",
      "|        0| 0.05813953488372093|     0|                   0|    0.0|           0| 1.0|           0.0|               0.0|0.6666666666666666|              0|(10,[1,6,9],[0.05...|\n",
      "|        0| 0.05813953488372093|     0|                   0|    0.0|           1| 1.0|           0.0|               0.0|               1.0|              0|(10,[1,5,6,9],[0....|\n",
      "|        0| 0.05813953488372093|     0|                   0|    0.0|           1| 1.0|           0.0|               0.0|               1.0|              0|(10,[1,5,6,9],[0....|\n",
      "|        0| 0.05813953488372093|     0|                   0|    0.0|           1| 1.0|           0.0|               0.0|               1.0|              0|(10,[1,5,6,9],[0....|\n",
      "|        0| 0.05813953488372093|     0|                   0|   0.25|           0| 0.0|           0.0|               1.0|               1.0|              0|(10,[1,4,8,9],[0....|\n",
      "|        0| 0.05813953488372093|     0|                   0|   0.25|           0| 0.5|           0.0|               0.0|               1.0|              0|(10,[1,4,6,9],[0....|\n",
      "|        0| 0.05813953488372093|     0|                   0|   0.25|           1| 0.5|           0.0|               1.0|               1.0|              0|[0.0,0.0581395348...|\n",
      "|        0| 0.05813953488372093|     0|                   0|   0.25|           1| 1.0|           0.0|               0.0|0.3333333333333333|              0|(10,[1,4,5,6,9],[...|\n",
      "|        0| 0.05813953488372093|     0|                   0|   0.25|           1| 1.0|           0.0|               0.0|               1.0|              0|(10,[1,4,5,6,9],[...|\n",
      "|        0| 0.05813953488372093|     0|                   0|   0.25|           1| 1.0|           0.0|               0.5|               0.0|              0|(10,[1,4,5,6,8],[...|\n",
      "+---------+--------------------+------+--------------------+-------+------------+----+--------------+------------------+------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CholCheck',\n",
       " 'BMI',\n",
       " 'Stroke',\n",
       " 'HeartDiseaseorAttack',\n",
       " 'GenHlth',\n",
       " 'PhysActivity',\n",
       " 'Diet',\n",
       " 'cardiovascular',\n",
       " 'unhealthy_behavior',\n",
       " 'healthcare',\n",
       " 'Diabetes_binary',\n",
       " 'features']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def get_hyperparameters(model):\n",
    "    best_model = model.bestModel\n",
    "\n",
    "    # Extract and print the best hyperparameters\n",
    "    best_params = best_model.extractParamMap()\n",
    "\n",
    "    # Print each parameter and its value\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param.name}: {value}\")\n",
    "\n",
    "        \n",
    "def get_metrics(predictions):\n",
    "    # Accuracy\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"Diabetes_binary\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "\n",
    "    # F1-Score\n",
    "    f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"Diabetes_binary\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "    f1_score = f1_evaluator.evaluate(predictions)\n",
    "\n",
    "    # Recall\n",
    "    recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"Diabetes_binary\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "    recall = recall_evaluator.evaluate(predictions)\n",
    "\n",
    "    # Precision\n",
    "    precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"Diabetes_binary\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "    precision = precision_evaluator.evaluate(predictions)\n",
    "\n",
    "    # Step 3: Print the evaluation results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-Score: {f1_score:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                00]\r"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(labelCol=\"Diabetes_binary\", featuresCol=\"features\")\n",
    "\n",
    "# Param grid for hyperparameter tuning\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation\n",
    "cv_lr = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid_lr,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5, parallelism=64)\n",
    "\n",
    "# Fit model\n",
    "lr_model = cv_lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: 2\n",
      "elasticNetParam: 0.0\n",
      "family: auto\n",
      "featuresCol: features\n",
      "fitIntercept: True\n",
      "labelCol: Diabetes_binary\n",
      "maxBlockSizeInMB: 0.0\n",
      "maxIter: 100\n",
      "predictionCol: prediction\n",
      "probabilityCol: probability\n",
      "rawPredictionCol: rawPrediction\n",
      "regParam: 0.01\n",
      "standardization: True\n",
      "threshold: 0.5\n",
      "tol: 1e-06\n"
     ]
    }
   ],
   "source": [
    "get_hyperparameters(lr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8615\n",
      "F1-Score: 0.8217\n",
      "Recall: 0.8615\n",
      "Precision: 0.8246\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_data)\n",
    "get_metrics(lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                0]0]]]\r"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"Diabetes_binary\", featuresCol=\"features\")\n",
    "\n",
    "paramGrid_dt = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 20]) \\\n",
    "    .addGrid(dt.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "cv_dt = CrossValidator(estimator=dt,\n",
    "                    estimatorParamMaps=paramGrid_dt,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5, parallelism=64)\n",
    "\n",
    "dt_model = cv_dt.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: False\n",
      "checkpointInterval: 10\n",
      "featuresCol: features\n",
      "impurity: gini\n",
      "labelCol: Diabetes_binary\n",
      "leafCol: \n",
      "maxBins: 64\n",
      "maxDepth: 10\n",
      "maxMemoryInMB: 256\n",
      "minInfoGain: 0.0\n",
      "minInstancesPerNode: 1\n",
      "minWeightFractionPerNode: 0.0\n",
      "predictionCol: prediction\n",
      "probabilityCol: probability\n",
      "rawPredictionCol: rawPrediction\n",
      "seed: 8624640193437594967\n"
     ]
    }
   ],
   "source": [
    "get_hyperparameters(dt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8603\n",
      "F1-Score: 0.8258\n",
      "Recall: 0.8603\n",
      "Precision: 0.8242\n"
     ]
    }
   ],
   "source": [
    "dt_predictions = dt_model.transform(test_data)\n",
    "get_metrics(dt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(labelCol=\"Diabetes_binary\", featuresCol=\"features\")\n",
    "\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "cv_rf = CrossValidator(estimator=rf,\n",
    "                    estimatorParamMaps=paramGrid_rf,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5, parallelism=32)\n",
    "\n",
    "rf_model = cv_rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hyperparameters(rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions = rf_model.transform(test_data)\n",
    "get_metrics(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient-Boosted Tree Classifier\n",
    "gbt = GBTClassifier(labelCol=\"Diabetes_binary\", featuresCol=\"features\", maxIter=100)\n",
    "\n",
    "paramGrid_gbt = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "cv_gbt = CrossValidator(estimator=gbt,\n",
    "                    estimatorParamMaps=paramGrid_gbt,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5, parallelism=64)\n",
    "\n",
    "gbt_model = cv_gbt.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hyperparameters(gbt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "get_metrics(gbt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svm = LinearSVC(labelCol=\"Diabetes_binary\", featuresCol=\"features\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(svm.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(estimator=svm,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5,  parallelism=64)\n",
    "\n",
    "svm_model = cv.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hyperparameters(svm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm_model.transform(test_data)\n",
    "get_metrics(svm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]]]]\r"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier\n",
    "nb = NaiveBayes(labelCol=\"Diabetes_binary\", featuresCol=\"features\")\n",
    "\n",
    "paramGrid_nb = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0.5, 1.0, 1.5]) \\\n",
    "    .build()\n",
    "\n",
    "cv_nb = CrossValidator(estimator=nb,\n",
    "                    estimatorParamMaps=paramGrid_nb,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5,  parallelism=64)\n",
    "\n",
    "nb_model = cv_nb.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuresCol: features\n",
      "labelCol: Diabetes_binary\n",
      "modelType: multinomial\n",
      "predictionCol: prediction\n",
      "probabilityCol: probability\n",
      "rawPredictionCol: rawPrediction\n",
      "smoothing: 1.5\n"
     ]
    }
   ],
   "source": [
    "get_hyperparameters(nb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2838:(163 + 37) / 200][Stage 2841:> (0 + 1) / 1][Stage 2843:(7 + 26) / 200]200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8598\n",
      "F1-Score: 0.8034\n",
      "Recall: 0.8598\n",
      "Precision: 0.8163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                00]\r"
     ]
    }
   ],
   "source": [
    "nb_predictions = nb_model.transform(test_data)\n",
    "get_metrics(nb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP classifier\n",
    "layers = [10, 16, 1]\n",
    "mlp = MultilayerPerceptronClassifier(labelCol=\"Diabetes_binary\", featuresCol=\"features\", layers=layers, seed=1234)\n",
    "\n",
    "paramGrid_mlp = ParamGridBuilder() \\\n",
    "    .addGrid(mlp.layers, [[len(X_pyspark.columns), 5, 4, 2], [len(X_pyspark.columns), 10, 5, 2]]) \\\n",
    "    .build()\n",
    "\n",
    "cv_mlp = CrossValidator(estimator=mlp,\n",
    "                    estimatorParamMaps=paramGrid_mlp,\n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol=\"Diabetes_binary\"),\n",
    "                    numFolds=5,  parallelism=64)\n",
    "\n",
    "mlp_model = cv_mlp.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hyperparameters(mlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_predictions = mlp_model.transform(test_data)\n",
    "get_metrics(mlp_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
